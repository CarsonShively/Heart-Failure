{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model - Random Forest"
      ],
      "metadata": {
        "id": "sffl7CeEHLIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    make_scorer,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import optuna\n",
        "from sklearn.utils.validation import check_is_fitted"
      ],
      "metadata": {
        "id": "_mwzKDPfHSXp"
      },
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "id": "q5AitOp5HCYv"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/CarsonShively/Heart-Failure/refs/heads/main/data/heart_failure.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=99\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering\n",
        "I explored several engineered features based on clinical relevance and interaction potential. After systematic testing, only the features that consistently improved cross-validation metrics were retained."
      ],
      "metadata": {
        "id": "8m0GLjIbvvkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self._transform_output = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        X['ef_to_creatinine'] = X['ejection_fraction'] / X['serum_creatinine']\n",
        "        X['ef_drop_per_age'] = (100 - X['ejection_fraction']) / X['age']\n",
        "        X['ef_per_time'] = X['ejection_fraction'] / X['time']\n",
        "        X['time_x_ef'] = X['time'] * X['ejection_fraction']\n",
        "        X['creatinine_x_ef'] = X['serum_creatinine'] * X['ejection_fraction']\n",
        "        X['time_x_creatinine'] = X['time'] * X['serum_creatinine']\n",
        "\n",
        "        X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        if self._transform_output == \"pandas\":\n",
        "            return pd.DataFrame(X, columns=X.columns, index=X.index)\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def set_output(self, transform=None):\n",
        "        self._transform_output = transform\n",
        "        return self\n",
        "\n",
        "FE_pipeline = Pipeline(steps=[\n",
        "    ('FE', FeatureEngineer()),\n",
        "])\n",
        "FE_pipeline.set_output(transform=\"pandas\")\n",
        "_ = FE_pipeline"
      ],
      "metadata": {
        "id": "rq1cILyDvyJl"
      },
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Features\n",
        " I evaluated multiple strategies for handling missing values, and imputing with the mode consistently yielded the best performance. After imputation, I filtered out any constant columns to prevent them from introducing noise or reducing model generalizability."
      ],
      "metadata": {
        "id": "04riHX36HuNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_cols = [\n",
        "    'anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking'\n",
        "]\n",
        "\n",
        "class BinaryInt64Cleaner(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, binary_cols):\n",
        "        self.binary_cols = binary_cols\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.binary_cols:\n",
        "            X[col] = pd.to_numeric(X[col], errors='coerce').astype('Int64')\n",
        "            X[col] = X[col].where(X[col].isin([0, 1]), pd.NA)\n",
        "        return X\n",
        "\n",
        "\n",
        "class BinaryImputer(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, binary_cols):\n",
        "        self.binary_cols = binary_cols\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.modes_ = {col: X[col].mode(dropna=True)[0] for col in self.binary_cols}\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.binary_cols:\n",
        "            X[col] = X[col].fillna(self.modes_[col])\n",
        "        return X\n",
        "\n",
        "class ConstantBinaryDropper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        self.constant_features_ = [col for col in self.columns if X[col].nunique(dropna=True) == 1]\n",
        "\n",
        "        if self.constant_features_:\n",
        "            print(f\"Dropping constant features: {self.constant_features_}\")\n",
        "        else:\n",
        "            print(\"No constant features dropped.\")\n",
        "\n",
        "        self.feature_names_out_ = [col for col in self.columns if col not in self.constant_features_]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        return X.drop(columns=self.constant_features_, errors='ignore')\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        return self.feature_names_out_\n",
        "\n",
        "\n",
        "binary_pipeline = Pipeline(steps=[\n",
        "    ('cleaner', BinaryInt64Cleaner(binary_cols=binary_cols)),\n",
        "    ('imputer', BinaryImputer(binary_cols=binary_cols)),\n",
        "    ('constant_cols', ConstantBinaryDropper(columns=binary_cols))\n",
        "])\n",
        "binary_pipeline.set_output(transform=\"pandas\")\n",
        "_ = binary_pipeline"
      ],
      "metadata": {
        "id": "aImj7NRCHwhr"
      },
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numeric features\n",
        "\n",
        "I first checked for low-variance features to reduce noise and eliminate uninformative inputs. I then checked for highly correlated feature pairs to simplify the dataset and minimize redundancy."
      ],
      "metadata": {
        "id": "JSKpGzyyJdjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = [\n",
        "    'age',\n",
        "    'creatinine_phosphokinase',\n",
        "    'ejection_fraction',\n",
        "    'platelets',\n",
        "    'serum_creatinine',\n",
        "    'serum_sodium',\n",
        "    'time',\n",
        "    'ef_to_creatinine',\n",
        "    'ef_drop_per_age',\n",
        "    'ef_per_time',\n",
        "    'time_x_ef',\n",
        "    'creatinine_x_ef',\n",
        "    'time_x_creatinine',\n",
        "]\n",
        "\n",
        "\n",
        "class CoerceToFloat(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.columns:\n",
        "            X[col] = X[col].astype(float)\n",
        "        return X\n",
        "\n",
        "class NumericImputer(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        self.medians_ = {\n",
        "            col: X[col].median(skipna=True)\n",
        "            for col in self.columns\n",
        "        }\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.columns:\n",
        "            X[col] = X[col].fillna(self.medians_[col])\n",
        "        return X\n",
        "\n",
        "class LowVarianceDropper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.0):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        variances = X.var()\n",
        "        self.low_variance_features_ = variances[variances <= self.threshold].index.tolist()\n",
        "\n",
        "        if self.low_variance_features_:\n",
        "            print(f\"Dropping low-variance features (threshold={self.threshold}): {self.low_variance_features_}\")\n",
        "        else:\n",
        "            print(f\"No low variance features dropped (threshold={self.threshold})\")\n",
        "\n",
        "        self.feature_names_out_ = [col for col in X.columns if col not in self.low_variance_features_]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        return X.drop(columns=self.low_variance_features_, errors='ignore')\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        return self.feature_names_out_\n",
        "\n",
        "class CorrelationReporter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, numeric_cols, threshold=0.9):\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.threshold = threshold\n",
        "        self._transform_output = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "\n",
        "        X_numeric = X[self.numeric_cols].select_dtypes(include='number')\n",
        "\n",
        "        corr_matrix = X_numeric.corr(method='pearson').abs()\n",
        "\n",
        "        upper = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )\n",
        "\n",
        "        self.high_corr_pairs_ = [\n",
        "            (col1, col2, upper.loc[col2, col1])\n",
        "            for col1 in upper.columns\n",
        "            for col2 in upper.index\n",
        "            if not pd.isna(upper.loc[col2, col1]) and upper.loc[col2, col1] >= self.threshold\n",
        "        ]\n",
        "\n",
        "        if self.high_corr_pairs_:\n",
        "            print(f\"\\n🔍 Highly correlated feature pairs (|r| ≥ {self.threshold}):\")\n",
        "            for a, b, r in sorted(self.high_corr_pairs_, key=lambda x: -x[2]):\n",
        "                print(f\"  {a} ↔ {b} | r = {r:.4f}\")\n",
        "        else:\n",
        "            print(f\"\\n✅ No feature pairs with |r| ≥ {self.threshold}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X\n",
        "\n",
        "    def set_output(self, transform=None):\n",
        "        self._transform_output = transform\n",
        "        return self\n",
        "\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('float', CoerceToFloat(columns=numeric_features)),\n",
        "    ('imputer', NumericImputer(columns=numeric_features)),\n",
        "    ('low_variance', LowVarianceDropper(threshold=0.001)),\n",
        "    ('correlation_report', CorrelationReporter(numeric_cols=numeric_features, threshold=0.9)),\n",
        "])\n",
        "\n",
        "numeric_pipeline.set_output(transform=\"pandas\")\n",
        "_ = numeric_pipeline"
      ],
      "metadata": {
        "id": "RBQ9KrhKJgg_"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline\n",
        "I ran Optuna with 30 trials using 5-fold cross-validation, optimizing specifically for recall, which was the primary objective. The best hyperparameters found from this study were hard-coded into the final model. Further optimization will be explored in future iterations to maximize performance further."
      ],
      "metadata": {
        "id": "TeUUGNlsLUGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer([\n",
        "    ('bin', binary_pipeline, binary_cols),\n",
        "    ('num', numeric_pipeline, numeric_features)\n",
        "])\n",
        "\n",
        "preprocessor.set_output(transform='pandas')\n",
        "\n",
        "best_rf = RandomForestClassifier(\n",
        "    n_estimators=238,\n",
        "    max_depth=8,\n",
        "    min_samples_split=6,\n",
        "    min_samples_leaf=7,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "full_pipeline = Pipeline([\n",
        "    ('feature_engineering', FeatureEngineer()),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', best_rf)\n",
        "])\n",
        "\n",
        "full_pipeline.set_output(transform=\"pandas\")\n",
        "_ = full_pipeline"
      ],
      "metadata": {
        "id": "26Xw3EwILXtO"
      },
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metrics"
      ],
      "metadata": {
        "id": "CGDKquoCP8fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "scores = cross_validate(\n",
        "    estimator=full_pipeline,\n",
        "    X=X_train,\n",
        "    y=y_train,\n",
        "    cv=cv,\n",
        "    scoring=scoring,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "cv_results = pd.DataFrame(scores)\n",
        "\n",
        "print(\"Cross-Validation Performance:\")\n",
        "for metric in scoring:\n",
        "    mean_score = cv_results[f'test_{metric}'].mean()\n",
        "    std_score = cv_results[f'test_{metric}'].std()\n",
        "    print(f\"{metric:>10}: {mean_score:.4f} ± {std_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSWsveEBP-r0",
        "outputId": "a9e2e1b9-97c0-4cc6-9f01-0f8ff4cfa70e"
      },
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No constant features dropped.\n",
            "No low variance features dropped (threshold=0.001)\n",
            "\n",
            "✅ No feature pairs with |r| ≥ 0.9\n",
            "No constant features dropped.\n",
            "No low variance features dropped (threshold=0.001)\n",
            "\n",
            "✅ No feature pairs with |r| ≥ 0.9\n",
            "No constant features dropped.\n",
            "No low variance features dropped (threshold=0.001)\n",
            "\n",
            "✅ No feature pairs with |r| ≥ 0.9\n",
            "No constant features dropped.\n",
            "No low variance features dropped (threshold=0.001)\n",
            "\n",
            "✅ No feature pairs with |r| ≥ 0.9\n",
            "No constant features dropped.\n",
            "No low variance features dropped (threshold=0.001)\n",
            "\n",
            "✅ No feature pairs with |r| ≥ 0.9\n",
            "Cross-Validation Performance:\n",
            "  accuracy: 0.8704 ± 0.0166\n",
            " precision: 0.7699 ± 0.0392\n",
            "    recall: 0.8592 ± 0.0793\n",
            "        f1: 0.8093 ± 0.0293\n",
            "   roc_auc: 0.9324 ± 0.0471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary after tuning and feature engineering\n",
        "1. No constant features were found among the binary features.\n",
        "\n",
        "2. No low-variance features were identified among the continuous numeric features.\n",
        "\n",
        "3. No feature correlations exceeded the 0.9 threshold, indicating low multicollinearity.\n",
        "\n",
        "After tuning and feature engineering the cross-validation performance improved significantly across all key metrics. Most notably, recall increased from 0.70 to 0.85, with a more consistent standard deviation — aligning well with the primary objective of improving sensitivity."
      ],
      "metadata": {
        "id": "JTXLphleN21v"
      }
    }
  ]
}