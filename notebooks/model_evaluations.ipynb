{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection\n",
        "\n",
        "The data strongly prioritized numeric features, many of which exhibited non-linear relationships. SHAP feature importance confirmed that the model valued the squared features, suggesting that a tree-based model would be well-suited to capturing these patterns without requiring explicit feature engineering.\n",
        "\n",
        "Models to test:\n",
        "1. Random forrest\n",
        "2. XGBoost\n",
        "3. LightGBM\n",
        "\n",
        "Given the small size of the dataset, I expect Random Forest to perform the best, as it typically generalizes better on limited data compared to boosting methods, which may overfit more easily under such constraints."
      ],
      "metadata": {
        "id": "SnPPUT3z6ieL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    make_scorer,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "v-YRSJNT6ffk"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "eUp9FRISpAgt"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/CarsonShively/Heart-Failure/refs/heads/main/data/heart_failure.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "y = df['DEATH_EVENT']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary features for tree baased models\n",
        "Binary features were already mapped to 0 and 1, so the only preprocessing step required was:\n",
        "\n",
        "NaN Handling: Missing values were imputed with safe defaults (e.g., 0 or mode where appropriate).\n",
        "\n",
        "This minimal preprocessing ensures clarity and consistency across tree-based model comparisons."
      ],
      "metadata": {
        "id": "et86RAbW7wZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_cols = [\n",
        "    'anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking'\n",
        "]\n",
        "\n",
        "zero_impute = [\n",
        "    'anaemia', 'diabetes', 'high_blood_pressure', 'smoking'\n",
        "]\n",
        "\n",
        "class BinaryInt64Cleaner(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, binary_cols):\n",
        "        self.binary_cols = binary_cols\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.binary_cols:\n",
        "            X[col] = pd.to_numeric(X[col], errors='coerce').astype('Int64')\n",
        "            X[col] = X[col].where(X[col].isin([0, 1]), pd.NA)\n",
        "        return X\n",
        "\n",
        "\n",
        "class BinaryImputer(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, binary_cols, sex_col='sex'):\n",
        "        self.binary_cols = binary_cols\n",
        "        self.sex_col = sex_col\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.sex_mode_ = X[self.sex_col].mode(dropna=True)[0]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.binary_cols:\n",
        "            X[col] = X[col].fillna(0)\n",
        "        X[self.sex_col] = X[self.sex_col].fillna(self.sex_mode_)\n",
        "        return X\n",
        "\n",
        "class ConvertToInt(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.columns:\n",
        "            X[col] = X[col].astype(int)\n",
        "        return X\n",
        "\n",
        "\n",
        "binary_pipeline = Pipeline(steps=[\n",
        "    ('cleaner', BinaryInt64Cleaner(binary_cols=binary_cols)),\n",
        "    ('imputer', BinaryImputer(binary_cols=zero_impute, sex_col='sex')),\n",
        "    ('to_int', ConvertToInt(columns=binary_cols))\n",
        "])\n",
        "binary_pipeline.set_output(transform=\"pandas\")\n",
        "_ = binary_pipeline"
      ],
      "metadata": {
        "id": "MSnW899272H0"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numeric features for tree based models\n",
        "To ensure a simple and consistent preprocessing pipeline for model comparison, I limited transformations to:\n",
        "\n",
        "1. NaN Handling: Missing values were imputed using the median.\n",
        "\n",
        "2. Outlier Treatment: Extreme values were clipped at the 1st and 99th percentiles.\n",
        "\n",
        "This avoids overfitting through complex transformations and keeps the pipeline fair across all models."
      ],
      "metadata": {
        "id": "1aEAJAuD8iJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = [\n",
        "    'age',\n",
        "    'creatinine_phosphokinase',\n",
        "    'ejection_fraction',\n",
        "    'platelets',\n",
        "    'serum_creatinine',\n",
        "    'serum_sodium',\n",
        "    'time',\n",
        "]\n",
        "\n",
        "class CoerceToFloat(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.columns:\n",
        "            X[col] = X[col].astype(float)\n",
        "        return X\n",
        "\n",
        "class NumericImputer(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        self.medians_ = {\n",
        "            col: X[col].median(skipna=True)\n",
        "            for col in self.columns\n",
        "        }\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.columns:\n",
        "            X[col] = X[col].fillna(self.medians_[col])\n",
        "        return X\n",
        "\n",
        "class OutlierClipper(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):\n",
        "    def __init__(self, columns, lower_quantile=0.01, upper_quantile=0.99):\n",
        "        self.columns = columns\n",
        "        self.lower_quantile = lower_quantile\n",
        "        self.upper_quantile = upper_quantile\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        self.lower_bounds_ = {\n",
        "            col: X[col].quantile(self.lower_quantile) for col in self.columns\n",
        "        }\n",
        "        self.upper_bounds_ = {\n",
        "            col: X[col].quantile(self.upper_quantile) for col in self.columns\n",
        "        }\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.columns:\n",
        "            X[col] = X[col].clip(self.lower_bounds_[col], self.upper_bounds_[col])\n",
        "        return X\n",
        "\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('float', CoerceToFloat(columns=numeric_features)),\n",
        "    ('imputer', NumericImputer(columns=numeric_features)),\n",
        "    ('clip', OutlierClipper(columns=numeric_features)),\n",
        "])\n",
        "\n",
        "numeric_pipeline.set_output(transform=\"pandas\")\n",
        "_ = numeric_pipeline"
      ],
      "metadata": {
        "id": "HJTwgwnD8mH7"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "Wv-eF3dx9EkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer([\n",
        "    ('bin', binary_pipeline, binary_cols),\n",
        "    ('num', numeric_pipeline, numeric_features)\n",
        "])\n",
        "\n",
        "preprocessor.set_output(transform='pandas')\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "tree_models = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=weights[1] / weights[0], verbosity=0, random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(class_weight='balanced', random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in tree_models.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    pipeline.set_output(transform='pandas')\n",
        "\n",
        "    scores = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=scoring)\n",
        "\n",
        "    summary = {\n",
        "        'Model': name,\n",
        "        **{f\"{metric} Mean\": scores[f\"test_{metric}\"].mean() for metric in scoring},\n",
        "        **{f\"{metric} Std\": scores[f\"test_{metric}\"].std() for metric in scoring}\n",
        "    }\n",
        "    results.append(summary)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeLLN3IU9Nup",
        "outputId": "38f6156d-6f81-49c1-e6dc-8750f8d6439e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Model  accuracy Mean  precision Mean  recall Mean   f1 Mean  \\\n",
            "0  Random Forest       0.853901        0.833846     0.701667  0.758867   \n",
            "1        XGBoost       0.841312        0.781822     0.715000  0.745710   \n",
            "2       LightGBM       0.832890        0.785761     0.701667  0.732190   \n",
            "\n",
            "   roc_auc Mean  accuracy Std  precision Std  recall Std    f1 Std  \\\n",
            "0      0.906395      0.053920       0.139600    0.050042  0.081171   \n",
            "1      0.905821      0.046204       0.096192    0.047770  0.066913   \n",
            "2      0.902520      0.052184       0.146104    0.088530  0.073878   \n",
            "\n",
            "   roc_auc Std  \n",
            "0     0.032229  \n",
            "1     0.027031  \n",
            "2     0.027179  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "As I expected, Random Forest outperformed both XGBoost and LightGBM on this small dataset. While LightGBM typically excels with larger data, it underperformed here. XGBoost was competitive but slightly less effective across recall and F1. Given the strong balance Random Forest showed, especially in generalization and recall, it is the optimal choice for the final model."
      ],
      "metadata": {
        "id": "Wg5nMsrLB_0i"
      }
    }
  ]
}