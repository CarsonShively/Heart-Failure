{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPieB9MJPuyBPBi4Lciwb6e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Model selection\n","\n","The data strongly prioritized numeric features, many of which exhibited non-linear relationships. SHAP feature importance confirmed that the model valued the squared features, suggesting that a tree-based model would be well-suited to capturing these patterns without requiring explicit feature engineering.\n","\n","Models to test:\n","1. Random forrest\n","2. XGBoost\n","3. LightGBM\n","\n","Given the small size of the dataset, I expect Random Forest to perform the best, as it typically generalizes better on limited data compared to boosting methods, which may overfit more easily under such constraints."],"metadata":{"id":"SnPPUT3z6ieL"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import warnings\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.model_selection import cross_validate, StratifiedKFold\n","from sklearn.metrics import (\n","    make_scorer,\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    roc_auc_score\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.utils.validation import check_is_fitted"],"metadata":{"id":"v-YRSJNT6ffk","executionInfo":{"status":"ok","timestamp":1753626480865,"user_tz":420,"elapsed":24,"user":{"displayName":"Carson Shively","userId":"11662509242748372059"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{"id":"eUp9FRISpAgt","executionInfo":{"status":"ok","timestamp":1753626482234,"user_tz":420,"elapsed":43,"user":{"displayName":"Carson Shively","userId":"11662509242748372059"}}},"outputs":[],"source":["url = \"https://raw.githubusercontent.com/CarsonShively/Heart-Failure/refs/heads/main/data/heart_failure.csv\"\n","df = pd.read_csv(url)\n","df.drop_duplicates(inplace=True)\n","\n","X = df.drop('DEATH_EVENT', axis=1)\n","y = df['DEATH_EVENT']\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X, y,\n","    test_size=0.2,\n","    stratify=y,\n","    random_state=42\n",")"]},{"cell_type":"markdown","source":["# Custom Casses"],"metadata":{"id":"vib8MYYFPHED"}},{"cell_type":"code","source":["class ConvertToInt64(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        X = pd.DataFrame(X)\n","        self.feature_names_in_ = X.columns.tolist()\n","        return self\n","\n","    def set_output(self, *, transform=None):\n","        return self\n","\n","    def transform(self, X):\n","        check_is_fitted(self)\n","        X = pd.DataFrame(X, columns=self.feature_names_in_).copy()\n","\n","        for col in X.columns:\n","            X[col] = pd.to_numeric(X[col], errors='coerce').astype(\"Int64\")\n","            X[col] = X[col].where(X[col].isin([0, 1]), pd.NA)\n","\n","        return pd.DataFrame(X, columns=self.feature_names_in_, index=X.index)\n","\n","\n","class ConvertToint64(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        X = pd.DataFrame(X)\n","        self.feature_names_in_ = X.columns.tolist()\n","        return self\n","\n","    def set_output(self, *, transform=None):\n","        return self\n","\n","    def transform(self, X):\n","        check_is_fitted(self)\n","        X = pd.DataFrame(X, columns=self.feature_names_in_).copy()\n","\n","        for col in X.columns:\n","            X[col] = X[col].astype(int)\n","\n","        return pd.DataFrame(X, columns=self.feature_names_in_, index=X.index)\n","\n","class CoerceToFloat(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        X = pd.DataFrame(X)\n","        self.feature_names_in_ = X.columns.tolist()\n","        return self\n","\n","    def set_output(self, *, transform=None):\n","        return self\n","\n","    def transform(self, X):\n","        check_is_fitted(self)\n","        X = pd.DataFrame(X, columns=self.feature_names_in_).copy()\n","\n","        for col in X.columns:\n","            X[col] = X[col].astype(float)\n","\n","        return pd.DataFrame(X, columns=self.feature_names_in_, index=X.index)\n","\n","class OutlierClipper(BaseEstimator, TransformerMixin):\n","    def __init__(self, columns=None, lower_quantile=0.01, upper_quantile=0.99):\n","        self.columns = columns\n","        self.lower_quantile = lower_quantile\n","        self.upper_quantile = upper_quantile\n","\n","    def fit(self, X, y=None):\n","        X = pd.DataFrame(X)\n","        self.feature_names_in_ = X.columns.tolist()\n","\n","        self.lower_bounds_ = {\n","            col: X[col].quantile(self.lower_quantile) for col in X.columns\n","        }\n","        self.upper_bounds_ = {\n","            col: X[col].quantile(self.upper_quantile) for col in X.columns\n","        }\n","        return self\n","\n","    def set_output(self, *, transform=None):\n","        return self\n","\n","    def transform(self, X):\n","        check_is_fitted(self)\n","        X = pd.DataFrame(X).copy()\n","\n","        for col in X.columns:\n","            X[col] = X[col].clip(self.lower_bounds_[col], self.upper_bounds_[col])\n","\n","        return pd.DataFrame(X, columns=self.feature_names_in_, index=X.index)"],"metadata":{"id":"iZ-ft5uiPRmT","executionInfo":{"status":"ok","timestamp":1753626482990,"user_tz":420,"elapsed":19,"user":{"displayName":"Carson Shively","userId":"11662509242748372059"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# Binary features for tree baased models\n","Binary features were already mapped to 0 and 1, so the only preprocessing step required was:\n","\n","NaN Handling: Missing values were imputed with safe defaults (e.g., 0 or mode where appropriate).\n","\n","This minimal preprocessing ensures clarity and consistency across tree-based model comparisons."],"metadata":{"id":"et86RAbW7wZA"}},{"cell_type":"code","source":["binary_pipeline = Pipeline(steps=[\n","    ('Int', ConvertToInt64()),\n","    ('impute', SimpleImputer(strategy='constant', fill_value=0)),\n","    ('int', ConvertToint64())\n","])\n","\n","sex_pipeline = Pipeline(steps=[\n","    ('Int', ConvertToInt64()),\n","    ('impute', SimpleImputer(strategy='most_frequent')),\n","    ('int', ConvertToint64())\n","])"],"metadata":{"id":"MSnW899272H0","executionInfo":{"status":"ok","timestamp":1753626485572,"user_tz":420,"elapsed":11,"user":{"displayName":"Carson Shively","userId":"11662509242748372059"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["# Numeric features for tree based models\n","To ensure a simple and consistent preprocessing pipeline for model comparison, I limited transformations to:\n","\n","1. NaN Handling: Missing values were imputed using the median.\n","\n","2. Outlier Treatment: Extreme values were clipped at the 1st and 99th percentiles.\n","\n","This avoids overfitting through complex transformations and keeps the pipeline fair across all models."],"metadata":{"id":"1aEAJAuD8iJv"}},{"cell_type":"code","source":["numeric_pipeline = Pipeline([\n","    ('float', CoerceToFloat()),\n","    ('impute', SimpleImputer(strategy='median')),\n","    ('clip', OutlierClipper()),\n","])"],"metadata":{"id":"HJTwgwnD8mH7","executionInfo":{"status":"ok","timestamp":1753626457263,"user_tz":420,"elapsed":11,"user":{"displayName":"Carson Shively","userId":"11662509242748372059"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Pipeline"],"metadata":{"id":"Wv-eF3dx9EkE"}},{"cell_type":"code","source":["binary_cols = [\n","    'anaemia', 'diabetes', 'high_blood_pressure', 'smoking'\n","]\n","\n","numeric_features = [\n","    'age',\n","    'creatinine_phosphokinase',\n","    'ejection_fraction',\n","    'platelets',\n","    'serum_creatinine',\n","    'serum_sodium',\n","    'time',\n","]\n","\n","preprocessor = ColumnTransformer([\n","    ('bin', binary_pipeline, binary_cols),\n","    ('sex', sex_pipeline, ['sex']),\n","    ('num', numeric_pipeline, numeric_features)\n","])\n","\n","preprocessor.set_output(transform='pandas')\n","\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n","\n","tree_models = {\n","    \"Random Forest\": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n","    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=weights[1] / weights[0], verbosity=0, random_state=42),\n","    \"LightGBM\": LGBMClassifier(class_weight='balanced', random_state=42, verbose=-1)\n","}\n","\n","scoring = {\n","    'accuracy': 'accuracy',\n","    'precision': 'precision',\n","    'recall': 'recall',\n","    'f1': 'f1',\n","    'roc_auc': 'roc_auc'\n","}\n","\n","results = []\n","\n","for name, model in tree_models.items():\n","    pipeline = Pipeline([\n","        ('preprocessor', preprocessor),\n","        ('classifier', model)\n","    ])\n","    pipeline.set_output(transform='pandas')\n","\n","    scores = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=scoring)\n","\n","    summary = {\n","        'Model': name,\n","        **{f\"{metric} Mean\": scores[f\"test_{metric}\"].mean() for metric in scoring},\n","        **{f\"{metric} Std\": scores[f\"test_{metric}\"].std() for metric in scoring}\n","    }\n","    results.append(summary)\n","\n","results_df = pd.DataFrame(results)\n","print(results_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeLLN3IU9Nup","executionInfo":{"status":"ok","timestamp":1753626490437,"user_tz":420,"elapsed":3629,"user":{"displayName":"Carson Shively","userId":"11662509242748372059"}},"outputId":"cc8454c0-a89e-488d-d412-039ef9d619cb"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["           Model  accuracy Mean  precision Mean  recall Mean   f1 Mean  \\\n","0  Random Forest       0.849645        0.843636     0.675000  0.745924   \n","1        XGBoost       0.841312        0.781822     0.715000  0.745710   \n","2       LightGBM       0.832890        0.785761     0.701667  0.732190   \n","\n","   roc_auc Mean  accuracy Std  precision Std  recall Std    f1 Std  \\\n","0      0.905236      0.047745       0.136405    0.043381  0.071081   \n","1      0.905821      0.046204       0.096192    0.047770  0.066913   \n","2      0.902520      0.052184       0.146104    0.088530  0.073878   \n","\n","   roc_auc Std  \n","0     0.025187  \n","1     0.027031  \n","2     0.027179  \n"]}]},{"cell_type":"markdown","source":["# Summary\n","\n","As I expected, Random Forest outperformed both XGBoost and LightGBM on this small dataset. While LightGBM typically excels with larger data, it underperformed here. XGBoost was competitive but slightly less effective across recall and F1. Given the strong balance Random Forest showed, especially in generalization and recall, it is the optimal choice for the final model."],"metadata":{"id":"Wg5nMsrLB_0i"}}]}